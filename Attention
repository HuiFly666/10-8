import torch  
import torch.nn as nn  
import torch.nn.functional as F  


class Attention(nn.Module):  
    def __init__(self, hidden_size):  
        super(Attention, self).__init__()  
        self.Wa = nn.Linear(hidden_size, hidden_size)  
        self.Ua = nn.Linear(hidden_size, hidden_size)  
        self.Va = nn.Linear(hidden_size, 1)  

    def forward(self, hidden_states):  
        scores = self.Va(torch.tanh(self.Wa(hidden_states) + self.Ua(hidden_states)))  
        attention_weights = F.softmax(scores, dim=1)  
        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), hidden_states).squeeze(1)  
        return context_vector, attention_weights  


import torch  
import torch.nn as nn  
import torch.nn.functional as F  

class Attention(nn.Module):  
    def __init__(self, hidden_size):  
        super(Attention, self).__init__()  
        self.Wa = nn.Linear(hidden_size, hidden_size)  
        self.Ua = nn.Linear(hidden_size, hidden_size)  
        self.Va = nn.Linear(hidden_size, 1)  

    def forward(self, hidden_states):  
        scores = self.Va(torch.tanh(self.Wa(hidden_states) + self.Ua(hidden_states)))  
        attention_weights = F.softmax(scores, dim=1)  
        context_vector = torch.bmm(attention_weights.permute(0, 2, 1), hidden_states).squeeze(1)  
        return context_vector, attention_weights  

class AudioClassifier(nn.Module):  
    def __init__(self, num_classes):  
        super(AudioClassifier, self).__init__()  
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))  
        
        # LSTM expects (input size, hidden size)  
        self.lstm = nn.LSTM(input_size=32, hidden_size=64, batch_first=True, bidirectional=True)  
        self.attention = Attention(hidden_size=128)  # 使用双向LSTM的隐藏层大小  
        self.fc = nn.Linear(128, num_classes)  

    def forward(self, x):  
        # x: (batch_size, 1, height, width) - 特征图的形状  
        x = F.relu(self.conv1(x))  
        x = F.max_pool2d(x, kernel_size=2)  
        x = F.relu(self.conv2(x))  
        x = F.max_pool2d(x, kernel_size=2)  

        # 假设经过卷积和池化后的特征图形状为 (batch_size, channels, height, width)  
        # 转换为 LSTM 输入格式，需要确保输入为 (batch_size, seq_len, feature_dim)  
        # 这里假设输出形状为 (32, 32, h, w)，需手动确定h和w  
        # 这里需要重塑为 (batch_size, seq_len, feature_dim) 格式  
        batch_size, channels, height, width = x.size()  
        x = x.permute(0, 2, 3, 1)  # (batch_size, height, width, channels)  
        x = x.reshape(batch_size, height * width, channels)  # 使得 seq_len = height * width, feature_dim=channels  
        
        lstm_out, _ = self.lstm(x)  

        # 应用注意力机制  
        context_vector, attention_weights = self.attention(lstm_out)  

        # 分类  
        out = self.fc(context_vector)  
        return out, attention_weights
