class AudioClassifier(nn.Module):  
    def __init__(self, num_classes):  
        super(AudioClassifier, self).__init__()  
        
        # 使用 1D 卷积处理时间序列数据  
        self.conv1 = nn.Conv1d(in_channels=23, out_channels=32, kernel_size=3, stride=1, padding=1)  # n_mels = 23  
        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)   
        
        # LSTM expects (input size, hidden size)  
        # input_size应为卷积后的输出通道数  
        self.lstm = nn.LSTM(input_size=64, hidden_size=128, batch_first=True, bidirectional=True)  # 64是conv2的输出通道数  
        
        # 注意力机制的隐藏层大小是双向的  
        self.attention = Attention(hidden_size=256)  # 2 * 128 是双向LSTM的输出  
        self.fc = nn.Linear(256, num_classes)  # 输出类别数  

    def forward(self, x):  
        # x: (batch_size, time, n_mels)   
        x = x.permute(0, 2, 1)  # (batch_size, n_mels, time) 转换为Conv1d的输入格式  
        
        # 卷积层  
        x = F.relu(self.conv1(x))  # 输出形状 (batch_size, 32, time)  
        x = F.max_pool1d(x, kernel_size=2, stride=2)  # 池化以降低维度  
        
        x = F.relu(self.conv2(x))  # 输出形状 (batch_size, 64, new_time)  
        x = F.max_pool1d(x, kernel_size=2, stride=2)  # 池化以降低维度  

        # 这里需要确保输入到LSTM的格式是 (batch_size, seq_len, feature_dim)  
        batch_size, channels, new_time = x.size()  
        x = x.permute(0, 2, 1)  # 转换为 (batch_size, new_time, channels)  
        
        lstm_out, _ = self.lstm(x)  # LSTM的输入格式  

        # 应用注意力机制  
        context_vector, attention_weights = self.attention(lstm_out)  

        # 分类  
        out = self.fc(context_vector)  
        return out, attention_weights 
